<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Qihuang Zhong (钟起煌)</title>
    <link rel="icon" type="image/x-icon" href="whu.png"/>
</head>

<body>
    <h1 style="padding-left: 1em">
        <img src="whu2.png" alt="WHU" style="width:auto; height:1.5em" />
         Qihuang Zhong (钟起煌)</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html">Home</a></div>
    <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
    <!-- <div class="menu-item"><a href="talk.html">Invited Talks</a></div>
    <div class="menu-item"><a href="service.html">Professional Services</a></div> -->
</td>
<td id="layout-content">

    <h1 style="margin-top: 0em">Publications</h1><br>
    <p>[ <a href="#conference">Conference Papers</a>,
         <a href="#journal">Journal Articles</a>,
         <a href="#preprint">Preprints</a> ]</p>
    <p> # denotes co-first authors. * denotes a corresponding author. <br>
        All my pulibations can be obtained <a href="https://scholar.google.com.hk/citations?user=YCL8gkYAAAAJ&hl=zh-CN" target="_blank">HERE</a>. </p>

    <div>
        <h2><hr><a name="conference"></a>Conference Papers</h2>
        <ol>
              <li><p>
                <font color="#0D4884"> Revisiting Token Dropping Strategy in Efficient BERT Pretraining </font> 
                [<a href="https://2023.aclweb.org" target="_blank">Paper</a>] <br>
                <b>Qihuang Zhong</b>, Liang Ding, Juhua Liu*, Xuebo Liu, Min Zhang, Bo Du*, Dacheng Tao <br>
                <img src="https://img.shields.io/badge/CCF-A-red"/> The Annual Meeting of the Association for Computational Linguistics, 2023 (ACL 2023) (CORE Rank A*)   
             </p></li>
             <li><p>
                <font color="#0D4884"> Self-Evolution Learning for Discriminative Language Model Pretraining </font> 
                [<a href="https://2023.aclweb.org" target="_blank">Paper</a>] <br>
                <b>Qihuang Zhong#</b>, Liang Ding#, Juhua Liu*, Bo Du*, Dacheng Tao <br>
                <img src="https://img.shields.io/badge/CCF-A-red"/> Findings of The Annual Meeting of the Association for Computational Linguistics, 2023 (ACL 2023) (CORE Rank A*)   
            </p></li>
             <li><p>
                <font color="#0D4884"> Token-Level Self-Evolution Training for Sequence-to-Sequence Learning </font> 
                [<a href="https://2023.aclweb.org" target="_blank">Paper</a>] <br>
                Keqin Peng#, Liang Ding#, <b>Qihuang Zhong</b>, Yuanxin Ouyang, Wenge Rong, Zhang Xiong, and Dacheng Tao.  <br>
                <img src="https://img.shields.io/badge/CCF-A-red"/> The Annual Meeting of the Association for Computational Linguistics, 2023 (ACL 2023) (CORE Rank A*)   
            </p></li>
            <li><p>
                <font color="#0D4884"> Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models </font> 
                [<a href="https://aclanthology.org/2022.findings-emnlp.300.pdf" target="_blank">Paper</a>] 
                [<a href="https://github.com/WHU-ZQH/FSAM4PLM" target="_blank">Code</a>]<img src="https://img.shields.io/github/stars/WHU-ZQH/FSAM4PLM?style=social"/> <br>
                <b>Qihuang Zhong</b>, Liang Ding, Li Shen, Peng Mi, Juhua Liu*, Bo Du*, Dacheng Tao <br>
                <img src="https://img.shields.io/badge/CCF-B-red"/> Findings of Association for Computational Linguistics: EMNLP 2022. (CORE Rank A)   
            </p></li>
            <li><p>
                <font color="#0D4884"> A Contrastive Cross-channel Data Augmentation Framework for Aspect-based Sentiment Analysis </font> 
                [<a href="https://aclanthology.org/2022.coling-1.581.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/pdf/2204.07832.pdf" target="_blank">ArXiv</a>] 
                [<a href="https://github.com/wangbing1416/C3DA" target="_blank">Code</a>]<img src="https://img.shields.io/github/stars/wangbing1416/C3DA?style=social"/> <br>
                Bing Wang, Liang Ding*, <b>Qihuang Zhong</b>, Ximing Li, Dacheng Tao <br>
                <img src="https://img.shields.io/badge/CCF-B-red"/> International Conference on Computational Linguistics (COLING 2022). (CORE Rank A)  
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="journal"></a>Journal Articles</h2>
        <ol>
             <li><p>
                <font color="#0D4884"> Knowledge graph augmented network towards multiview representation learning for aspect-based sentiment analysis </font>
                [<a href="https://ieeexplore.ieee.org/document/10056277/" target="_blank">Paper</a>] 
                 [<a href="https://github.com/WHU-ZQH/KGAN" target="_blank">Code</a>]<img src="https://img.shields.io/github/stars/WHU-ZQH/KGAN?style=social"/> <br>
                <b>Qihuang Zhong</b>, Liang Ding, Juhua Liu*, Bo Du*, Hua Jin, Dacheng Tao <br>
                <img src="https://img.shields.io/badge/CCF-A-red"/> <img src="https://img.shields.io/badge/SCI-II-blue"/> IEEE Transactions on Knowledge and Data Engineering, 2023. (CORE Rank A*)
            </p></li>
             <li><p>
                <font color="#0D4884"> Joint image and feature adaptative attention-aware networks for cross-modality semantic segmentation </font>
                [<a href="https://link.springer.com/article/10.1007/s00521-021-06064-w" target="_blank">Paper</a>] <br>
                <b>Qihuang Zhong</b>, Fanzhou Zeng, Fei Liao*, Juhua Liu*, Bo Du, Jedi S Shang <br>
                <img src="https://img.shields.io/badge/CCF-C-red"/> <img src="https://img.shields.io/badge/SCI-II-blue"/> Neural Computing and Applications, 2023. (CORE Rank B)
            </p></li>
            <li><p>
                <font color="#0D4884"> SemiText: Scene text detection with semi-supervised learning </font>
                 [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220308948" target="_blank">Paper</a>] <br>
                Juhua Liu, <b>Qihuang Zhong</b>, Yuan Yuan, Hai Su, Bo Du* <br>
                <img src="https://img.shields.io/badge/CCF-C-red"/> <img src="https://img.shields.io/badge/SCI-II-blue"/> Neurocomputing, 2020. (CORE Rank B) 
            </p></li>
           
        </ol>
    </div>

     <div>
        <h2><hr><a name="preprint"></a>Pre-prints &amp; Reports</h2>
         <ol>
          <li><p>         
            [03/2023] <font color="#0D4884"> Towards Making the Most of ChatGPT for Machine Translation </font> 
            [<a href="https://arxiv.org/pdf/2303.13780.pdf" target="_blank">Report</a>] <br>
            Keqin Peng, Liang Ding, <b>Qihuang Zhong</b>, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao 
         </p></li>
          <li><p>         
            [03/2023] <font color="#0D4884"> AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks </font> 
            [<a href="https://arxiv.org/pdf/2303.00565.pdf" target="_blank">Arxiv</a>] 
            <br>
            Hao Sun, Li Shen, <b>Qihuang Zhong</b>, Liang Ding, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, Dacheng Tao
         </p></li>
         <li><p>         
            [02/2023] <font color="#0D4884"> Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT </font> 
            [<a href="https://arxiv.org/pdf/2302.10198.pdf" target="_blank">Report</a>] 
                [<a href="https://github.com/WHU-ZQH/ChatGPT-vs.-BERT" target="_blank">Code</a>]<img src="https://img.shields.io/github/stars/WHU-ZQH/ChatGPT-vs.-BERT?style=social"/> <br>
            <b>Qihuang Zhong</b>, Liang Ding, Juhua Liu*, Bo Du*, Dacheng Tao 
         </p></li>
         <li><p>         
            [02/2023] <font color="#0D4884"> Bag of Tricks for Effective Language Model Pretraining and Downstream Adaptation: A Case Study on GLUE </font> 
            [<a href="https://arxiv.org/pdf/2302.09268.pdf" target="_blank">Report</a>] [<a href="https://gluebenchmark.com/leaderboard/" target="_blank">GLUE leaderboard</a>] <br>
            <b>Qihuang Zhong</b>#, Liang Ding#, Keqin Peng, Juhua Liu*, Bo Du*, Li Shen, Yibing Zhan, Dacheng Tao 
         </p></li>
         <li><p>         
            [12/2022] <font color="#0D4884"> Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE </font> 
            [<a href="https://arxiv.org/pdf/2212.01853.pdf" target="_blank">Report</a>] [<a href="https://super.gluebenchmark.com/leaderboard/" target="_blank">SuperGLUE leaderboard</a>] <br>
            <b>Qihuang Zhong</b>#, Liang Ding#, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu*, Baosheng Yu, Bo Du*, Yixin Chen, Xinbo Gao, Chunyan Miao, Xiaoou Tang, Dacheng Tao 
         </p></li>
          <li><p>         
            [08/2022] <font color="#0D4884"> Panda: Prompt transfer meets knowledge distillation for efficient model adaptation </font> 
            [<a href="https://arxiv.org/pdf/2208.10160.pdf" target="_blank">Arxiv</a>] <br>
            <b>Qihuang Zhong</b>, Liang Ding, Juhua Liu*, Bo Du*, Dacheng Tao 
         </p></li>
          <li><p>         
            [05/2022] <font color="#0D4884"> E2S2: Encoding-enhanced sequence-to-sequence pretraining for language understanding and generation </font> 
            [<a href="https://arxiv.org/pdf/2205.14912.pdf" target="_blank">Arxiv</a>] [<a href="https://github.com/WHU-ZQH/E2S2" target="_blank">Code</a>]
             <img src="https://img.shields.io/github/stars/WHU-ZQH/E2S2?style=social"/> 
             <br>
            <b>Qihuang Zhong</b>, Liang Ding, Juhua Liu*, Bo Du*, Dacheng Tao <br>
             Status: IEEE TKDE (<i>under review</i>)
         </p></li>
         <li><p>         
            [10/2021] <font color="#0D4884"> Unified instance and knowledge alignment pretraining for aspect-based sentiment analysis </font> 
            [<a href="https://arxiv.org/pdf/2110.13398.pdf" target="_blank">Arxiv</a>] 
            [<a href="https://github.com/WHU-ZQH/UIKA" target="_blank">Code</a>]<img src="https://img.shields.io/github/stars/WHU-ZQH/UIKA?style=social"/> 
             <br>
            Juhua Liu#, <b>Qihuang Zhong</b>#, Liang Ding, Hua Jin, Bo Du*, Dacheng Tao <br>
            Status: IEEE TASLP (<i>MINOR REVISIONS for ENGLISH USAGE before acceptance</i>)
         </p></li>
        </ol>
    </div>
    
</td>
</tr>
</table>
</body>
</html>
